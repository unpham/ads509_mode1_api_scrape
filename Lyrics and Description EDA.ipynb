{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import string\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/lamnguyen/Dropbox/Uyen bom/USD/ADS-509/mod1/ads509_mode1_api_scrape/\"\n",
    "# data folder\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "408d9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, calculate and print the number of tokens, number of unique tokens,\n",
    "    number of characters, lexical diversity, and the most common tokens. Return a list with\n",
    "    the calculated statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "        \n",
    "        # Print the 5 most common tokens\n",
    "        common_tokens = Counter(tokens).most_common(num_tokens)\n",
    "        print(\"The most common tokens are:\")\n",
    "        for token, count in common_tokens[:5]: #print out 5 most common token\n",
    "            print(f\"{token}: {count}\")\n",
    "    \n",
    "    return [num_tokens, num_unique_tokens, lexical_diversity, num_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The most common tokens are:\n",
      "text: 3\n",
      "here: 2\n",
      "example: 2\n",
      "is: 1\n",
      "some: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: The assertion statements are used to raise errors if the conditions are not met. In this case, it will raise error if the discriptive_stat claculation came out different than the expected values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b850a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "#Create list lyric data list\n",
    "lyrics_data = []\n",
    "\n",
    "# Set path to the artist folders\n",
    "for artist_folder in os.listdir(data_location + lyrics_folder):\n",
    "    artist_path = os.path.join(data_location + lyrics_folder, artist_folder)\n",
    "    \n",
    "    # set path to the lyric file\n",
    "    for lyric_file in os.listdir(artist_path):\n",
    "        lyric_path = os.path.join(artist_path, lyric_file)\n",
    "            \n",
    "        # Read the lyrics from the lyric files\n",
    "        with open(lyric_path, 'r', encoding='utf8') as infile:\n",
    "            lyrics = infile.read()\n",
    "\n",
    "        # Extract the title from the lyrics\n",
    "        title = lyrics.split(\"\\n\", 1)[0].strip()\n",
    "\n",
    "        # Remove the title from the lyrics\n",
    "        lyrics = lyrics.replace(title, \"\").strip()\n",
    "\n",
    "        # Append the artist, title, and lyrics to the data list\n",
    "        lyrics_data.append([artist_folder, title, lyrics])\n",
    "\n",
    "# Create a DataFrame from the lyrics data\n",
    "lyric_df = pd.DataFrame(lyrics_data, columns=['artist', 'title', 'lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "db367068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Include Me Out\"</td>\n",
       "      <td>It is really very simple\\nJust a single pulse,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Electric\"</td>\n",
       "      <td>Electric...\\n\\nIt's electric\\nIt's a natural h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Beach 2K20\"</td>\n",
       "      <td>(So you wanna go out?\\nHow you gonna get there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Love Kills\"</td>\n",
       "      <td>If you're looking for love\\nGet a heart made o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Time Machine\"</td>\n",
       "      <td>Hey, what did I do?\\nCan't believe the fit I j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist             title                                             lyrics\n",
       "0  robyn  \"Include Me Out\"  It is really very simple\\nJust a single pulse,...\n",
       "1  robyn        \"Electric\"  Electric...\\n\\nIt's electric\\nIt's a natural h...\n",
       "2  robyn      \"Beach 2K20\"  (So you wanna go out?\\nHow you gonna get there...\n",
       "3  robyn      \"Love Kills\"  If you're looking for love\\nGet a heart made o...\n",
       "4  robyn    \"Time Machine\"  Hey, what did I do?\\nCan't believe the fit I j..."
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d2ff8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "twitter_files = os.listdir(data_location + twitter_folder)\n",
    "desc_files = [f for f in twitter_files if \"followers_data\" in f]\n",
    "twitter_data = defaultdict(list)\n",
    "for f in desc_files :\n",
    "    artist = f.split(\"_\")[0]\n",
    "        \n",
    "    with open(data_location + twitter_folder + f,'r', encoding='utf8') as infile :\n",
    "        next(infile)\n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if len(line) == 7 :\n",
    "                twitter_data[artist].append(line[6])\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "twit_df = pd.DataFrame.from_dict(twitter_data, orient='index')\n",
    "# Transpose the DataFrame to have artists as columns\n",
    "twit_df = twit_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0d1bb591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cher</th>\n",
       "      <th>robynkonichiwa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>\"I love chill\" •Facebook / Instagram / SoundCl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>books, movies, music, nature &amp; TV shows. OG Sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csu</td>\n",
       "      <td>(Am)auteur en herbe 🌱 - juriste en paille 🤡 - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>This Twitter profile is full of sarcasm and ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m unemployed and live with my parents. MOOPS!</td>\n",
       "      <td>Flora Youssef - Blogger &amp; Founder Posting revi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                cher  \\\n",
       "0           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜   \n",
       "1          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   \n",
       "2                                                csu   \n",
       "3  Writer @Washinformer @SpelmanCollege alumna #D...   \n",
       "4    I’m unemployed and live with my parents. MOOPS!   \n",
       "\n",
       "                                      robynkonichiwa  \n",
       "0  \"I love chill\" •Facebook / Instagram / SoundCl...  \n",
       "1  books, movies, music, nature & TV shows. OG Sw...  \n",
       "2  (Am)auteur en herbe 🌱 - juriste en paille 🤡 - ...  \n",
       "3  This Twitter profile is full of sarcasm and ra...  \n",
       "4  Flora Youssef - Blogger & Founder Posting revi...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "93611f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# Function to clean and tokenize the lyrics\n",
    "def clean_tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    # Remove punctuation\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "47b456ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cher</th>\n",
       "      <th>robynkonichiwa</th>\n",
       "      <th>cleaned_cher</th>\n",
       "      <th>cleaned_robynkonichiwa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>\"I love chill\" •Facebook / Instagram / SoundCl...</td>\n",
       "      <td>[𝙿𝚛𝚘𝚞𝚍, 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛, 𝚘𝚏, 𝚖𝚎𝚜𝚜𝚢, 𝚋𝚞𝚗𝚜, 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜]</td>\n",
       "      <td>[love, chill, •, facebook, instagram, soundclo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>books, movies, music, nature &amp; TV shows. OG Sw...</td>\n",
       "      <td>[163, ㎝／, 愛かっぷ, 💜, 26歳, 🍒, 工〇好きな女の子, 💓, フォローして...</td>\n",
       "      <td>[books, movies, music, nature, tv, shows, og, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csu</td>\n",
       "      <td>(Am)auteur en herbe 🌱 - juriste en paille 🤡 - ...</td>\n",
       "      <td>[csu]</td>\n",
       "      <td>[amauteur, en, herbe, 🌱, juriste, en, paille, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>This Twitter profile is full of sarcasm and ra...</td>\n",
       "      <td>[writer, washinformer, spelmancollege, alumna,...</td>\n",
       "      <td>[twitter, profile, full, sarcasm, rants, occas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m unemployed and live with my parents. MOOPS!</td>\n",
       "      <td>Flora Youssef - Blogger &amp; Founder Posting revi...</td>\n",
       "      <td>[’, unemployed, live, parents, moops]</td>\n",
       "      <td>[flora, youssef, blogger, founder, posting, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                cher  \\\n",
       "0           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜   \n",
       "1          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   \n",
       "2                                                csu   \n",
       "3  Writer @Washinformer @SpelmanCollege alumna #D...   \n",
       "4    I’m unemployed and live with my parents. MOOPS!   \n",
       "\n",
       "                                      robynkonichiwa  \\\n",
       "0  \"I love chill\" •Facebook / Instagram / SoundCl...   \n",
       "1  books, movies, music, nature & TV shows. OG Sw...   \n",
       "2  (Am)auteur en herbe 🌱 - juriste en paille 🤡 - ...   \n",
       "3  This Twitter profile is full of sarcasm and ra...   \n",
       "4  Flora Youssef - Blogger & Founder Posting revi...   \n",
       "\n",
       "                                        cleaned_cher  \\\n",
       "0      [𝙿𝚛𝚘𝚞𝚍, 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛, 𝚘𝚏, 𝚖𝚎𝚜𝚜𝚢, 𝚋𝚞𝚗𝚜, 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜]   \n",
       "1  [163, ㎝／, 愛かっぷ, 💜, 26歳, 🍒, 工〇好きな女の子, 💓, フォローして...   \n",
       "2                                              [csu]   \n",
       "3  [writer, washinformer, spelmancollege, alumna,...   \n",
       "4              [’, unemployed, live, parents, moops]   \n",
       "\n",
       "                              cleaned_robynkonichiwa  \n",
       "0  [love, chill, •, facebook, instagram, soundclo...  \n",
       "1  [books, movies, music, nature, tv, shows, og, ...  \n",
       "2  [amauteur, en, herbe, 🌱, juriste, en, paille, ...  \n",
       "3  [twitter, profile, full, sarcasm, rants, occas...  \n",
       "4  [flora, youssef, blogger, founder, posting, re...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean twitter data here\n",
    "# Apply the cleaning and tokenization function to the twitter columns\n",
    "twit_df['cleaned_cher'] = twit_df['cher'].apply(clean_tokenize)\n",
    "twit_df['cleaned_robynkonichiwa'] = twit_df['robynkonichiwa'].apply(clean_tokenize)\n",
    "twit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Include Me Out\"</td>\n",
       "      <td>It is really very simple\\nJust a single pulse,...</td>\n",
       "      <td>[really, simple, single, pulse, repeated, regu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Electric\"</td>\n",
       "      <td>Electric...\\n\\nIt's electric\\nIt's a natural h...</td>\n",
       "      <td>[electric, electric, natural, high, electric, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Beach 2K20\"</td>\n",
       "      <td>(So you wanna go out?\\nHow you gonna get there...</td>\n",
       "      <td>[wanna, go, gonna, get, ok, call, someone, alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Love Kills\"</td>\n",
       "      <td>If you're looking for love\\nGet a heart made o...</td>\n",
       "      <td>[youre, looking, love, get, heart, made, steel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Time Machine\"</td>\n",
       "      <td>Hey, what did I do?\\nCan't believe the fit I j...</td>\n",
       "      <td>[hey, cant, believe, fit, threw, stupid, wante...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist             title                                             lyrics  \\\n",
       "0  robyn  \"Include Me Out\"  It is really very simple\\nJust a single pulse,...   \n",
       "1  robyn        \"Electric\"  Electric...\\n\\nIt's electric\\nIt's a natural h...   \n",
       "2  robyn      \"Beach 2K20\"  (So you wanna go out?\\nHow you gonna get there...   \n",
       "3  robyn      \"Love Kills\"  If you're looking for love\\nGet a heart made o...   \n",
       "4  robyn    \"Time Machine\"  Hey, what did I do?\\nCan't believe the fit I j...   \n",
       "\n",
       "                                      cleaned_lyrics  \n",
       "0  [really, simple, single, pulse, repeated, regu...  \n",
       "1  [electric, electric, natural, high, electric, ...  \n",
       "2  [wanna, go, gonna, get, ok, call, someone, alr...  \n",
       "3  [youre, looking, love, get, heart, made, steel...  \n",
       "4  [hey, cant, believe, fit, threw, stupid, wante...  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean lyrics data here\n",
    "# Apply the cleaning and tokenization function to the lyrics column\n",
    "lyric_df['cleaned_lyrics'] = lyric_df['lyrics'].apply(clean_tokenize)\n",
    "lyric_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls to descriptive_stats here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c1b12a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15170 tokens in the data.\n",
      "There are 2157 unique tokens in the data.\n",
      "There are 72795 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n",
      "The most common tokens are:\n",
      "know: 305\n",
      "im: 299\n",
      "dont: 297\n",
      "love: 269\n",
      "got: 250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15170, 2157, 0.14218852999340803, 72795]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls descriptive_stats here for Robyn's lyric data\n",
    "descriptive_stats(lyric_df.loc[lyric_df['artist'] == 'robyn', 'cleaned_lyrics'].explode().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fcd745b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35266 tokens in the data.\n",
      "There are 3690 unique tokens in the data.\n",
      "There are 169232 characters in the data.\n",
      "The lexical diversity is 0.105 in the data.\n",
      "The most common tokens are:\n",
      "love: 966\n",
      "im: 511\n",
      "know: 480\n",
      "dont: 430\n",
      "youre: 332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[35266, 3690, 0.10463335790846708, 169232]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls descriptive_stats here for Cher's lyric data \n",
    "descriptive_stats(lyric_df.loc[lyric_df['artist'] == 'cher', 'cleaned_lyrics'].explode().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9e3dc437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1528078 tokens in the data.\n",
      "There are 241824 unique tokens in the data.\n",
      "There are 9108181 characters in the data.\n",
      "The lexical diversity is 0.158 in the data.\n",
      "The most common tokens are:\n",
      "music: 15362\n",
      "love: 11828\n",
      "im: 9098\n",
      "och: 7924\n",
      "life: 7558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1528078, 241824, 0.15825370170894418, 9108181]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calls descriptive_stats here for Robyn's twitter data \n",
    "#Robyn's data seemed to have float typed token causing trouble in descriptive_stat calculation\n",
    "tokens = twit_df['cleaned_robynkonichiwa'].explode().tolist()\n",
    "\n",
    "#filter out float typed token and convert the remain to string to avoid error\n",
    "tokens = [str(token) for token in tokens if not isinstance(token, float)]\n",
    "descriptive_stats(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8012562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16190060 tokens in the data.\n",
      "There are 1336852 unique tokens in the data.\n",
      "There are 92628042 characters in the data.\n",
      "The lexical diversity is 0.083 in the data.\n",
      "The most common tokens are:\n",
      "love: 217711\n",
      "im: 139895\n",
      "life: 126473\n",
      "music: 90160\n",
      "’: 85846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16190060, 1336852, 0.0825723931844601, 92628042]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calls descriptive_stats here for Cher's twitter data \n",
    "#Cher's data seemed to have float typed token causing trouble in descriptive_stat calculation\n",
    "tokens = twit_df['cleaned_cher'].explode().tolist()\n",
    "\n",
    "#filter out float typed token and convert the remain to string to avoid error\n",
    "tokens = [str(token) for token in tokens if not isinstance(token, float)]\n",
    "descriptive_stats(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: Stopwords are words that are very common and would have high frequncy in the data. Therefore, if stopwords were left in the data, they would dominate the list of most common words. The result of top 5  mostcommon words might have been stopwords instead of other useful words.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: My prior beliefs about the lexical diversity of lyrics between the artists was that there could be differences attributed to their distinct musical styles and the trending eras in which they were popular. Along with that, I also believed that the twitter lexical diversity might be influenced by the musical preferences, life perspectives, and writing styles of their respective fan bases, thereby leading to variations. The result showing differences in lexical diversity between the artists conformed these beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify the is_emoji() function correctly identifies emojis.\n",
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fdcbfeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Shorter approach:\n",
    "# emojis = defaultdict(list)\n",
    "# for artist in twitter_data:\n",
    "#     for desc in twitter_data[artist]:\n",
    "#         emojis[artist].extend([c for c in desc if emoji.is_emoji(c)])\n",
    "        \n",
    "# for artist in emojis:\n",
    "#     print(artist)\n",
    "#     print(Counter(emojis[artist]).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "567ef689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 common emojis by Robyn:\n",
      "❤ 4783\n",
      "🌈 4685\n",
      "🏳 3528\n",
      "♥ 3103\n",
      "✨ 2223\n",
      "🇺 1343\n",
      "🇪 1264\n",
      "🇸 1214\n",
      "✌ 1189\n",
      "♡ 1123\n",
      "Cher's top 10 emojis:\n",
      "❤ 79223\n",
      "🌈 47549\n",
      "♥ 33978\n",
      "🏳 33412\n",
      "✨ 29468\n",
      "🇺 25793\n",
      "💙 21379\n",
      "🌊 20223\n",
      "♡ 19432\n",
      "🇸 18309\n"
     ]
    }
   ],
   "source": [
    "def get_common_emojis(df, artist_column):\n",
    "    emojis_count = Counter()\n",
    "    for description in df[artist_column]:\n",
    "        if description is not None:\n",
    "            emojis = [c for c in description if is_emoji(c)]\n",
    "            emojis_count.update(emojis)\n",
    "\n",
    "    most_common_emojis = emojis_count.most_common(10)\n",
    "    return most_common_emojis\n",
    "\n",
    "#print out 10 most common emojis from Robyn's followers\n",
    "most_common_emojis = get_common_emojis(twit_df, 'robynkonichiwa')\n",
    "print(\"Top 10 common emojis by Robyn:\")\n",
    "for emoji, count in most_common_emojis:\n",
    "    print(emoji, count)\n",
    "\n",
    "#print out 10 most common emojis from Cher's followers\n",
    "most_common_emojis = get_common_emojis(twit_df, 'cher')\n",
    "print(\"Cher's top 10 emojis:\")\n",
    "for emoji, count in most_common_emojis:\n",
    "    print(emoji, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "df871bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robyn's top 10 hashtags:\n",
      "BlackLivesMatter 337\n",
      "BLM 306\n",
      "blacklivesmatter 208\n",
      "1 199\n",
      "music 174\n",
      "Music 113\n",
      "EDM 86\n",
      "LGBTQ 75\n",
      "TeamFollowBack 59\n",
      "blm 56\n",
      "----------------------------\n",
      "Cher's top 10 hashtags:\n",
      "BLM 9535\n",
      "Resist 6036\n",
      "BlackLivesMatter 4681\n",
      "resist 3797\n",
      "FBR 3239\n",
      "TheResistance 2995\n",
      "blacklivesmatter 2645\n",
      "1 2627\n",
      "Resistance 1919\n",
      "RESIST 1823\n"
     ]
    }
   ],
   "source": [
    "#Create function to get common hashtags\n",
    "def get_common_hashtags(df, artist_column):\n",
    "    hashtags_count = Counter() #create list of hashtags and their counts\n",
    "    for descriptions in df[artist_column]:\n",
    "        if isinstance(descriptions, str):  # Check if description is a string\n",
    "            hashtags = re.findall(r'#(\\w+)', descriptions, flags=re.IGNORECASE)  # Extract hashtags using regex\n",
    "            hashtags_count.update(hashtags)\n",
    "\n",
    "    most_common_hashtags = hashtags_count.most_common(10)\n",
    "    return most_common_hashtags\n",
    "\n",
    "#Top 10 most hashtags from Robyn's followers\n",
    "most_common_hashtags_robyn = get_common_hashtags(twit_df, 'robynkonichiwa')\n",
    "print(\"Robyn's top 10 hashtags:\")\n",
    "for hashtag, count in most_common_hashtags_robyn:\n",
    "    print(hashtag, count)\n",
    "\n",
    "print('----------------------------')\n",
    "    \n",
    "#Top 10 most hashtags from Cher's followers\n",
    "most_common_hashtags_cher = get_common_hashtags(twit_df, 'cher')\n",
    "print(\"Cher's top 10 hashtags:\")\n",
    "for hashtag, count in most_common_hashtags_cher:\n",
    "    print(hashtag, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7091a6",
   "metadata": {},
   "source": [
    "#### --> it might be more informative to observe most common words with stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f908504d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Include Me Out\"</td>\n",
       "      <td>It is really very simple\\nJust a single pulse,...</td>\n",
       "      <td>[really, simple, single, pulse, repeated, regu...</td>\n",
       "      <td>[include]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Electric\"</td>\n",
       "      <td>Electric...\\n\\nIt's electric\\nIt's a natural h...</td>\n",
       "      <td>[electric, electric, natural, high, electric, ...</td>\n",
       "      <td>[electric]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist             title                                             lyrics  \\\n",
       "0  robyn  \"Include Me Out\"  It is really very simple\\nJust a single pulse,...   \n",
       "1  robyn        \"Electric\"  Electric...\\n\\nIt's electric\\nIt's a natural h...   \n",
       "\n",
       "                                      cleaned_lyrics cleaned_title  \n",
       "0  [really, simple, single, pulse, repeated, regu...     [include]  \n",
       "1  [electric, electric, natural, high, electric, ...    [electric]  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the cleaning and tokenization function to the lyrics title\n",
    "lyric_df['cleaned_title'] = lyric_df['title'].apply(clean_tokenize)\n",
    "lyric_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "363802a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robyn's top 5 most common words in song titles:\n",
      "love 6\n",
      "dont 4\n",
      "u 4\n",
      "hang 3\n",
      "tell 3\n",
      "----------------------------\n",
      "Cher's top 5 most common words in song titles:\n",
      "love 38\n",
      "man 12\n",
      "song 11\n",
      "dont 10\n",
      "come 7\n"
     ]
    }
   ],
   "source": [
    "#Create function to get most common words in song titles \n",
    "def get_common_words(df, artist_name, title_column):\n",
    "    words_count = Counter()\n",
    "    artist_df = df[df['artist'] == artist_name]\n",
    "    for words in artist_df[title_column]:\n",
    "        for word in words:\n",
    "            words_count[word] += 1\n",
    "\n",
    "    most_common_words = words_count.most_common(5)\n",
    "    return most_common_words\n",
    "\n",
    "#Top 5 most common words in song titles by Robyn\n",
    "most_common_words = get_common_words(lyric_df, 'robyn', 'cleaned_title')\n",
    "print(\"Robyn's top 5 most common words in song titles:\")\n",
    "for word, count in most_common_words:\n",
    "    print(word, count)\n",
    "\n",
    "print('----------------------------') \n",
    "    \n",
    "#Top 5 most common words in song titles by Cher\n",
    "most_common_words = get_common_words(lyric_df, 'cher', 'cleaned_title')\n",
    "print(\"Cher's top 5 most common words in song titles:\")\n",
    "for word, count in most_common_words:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49aa95",
   "metadata": {},
   "source": [
    "#### Observe most common words including stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4a6d4cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robyn's top 5 most common words in song titles:\n",
      "me 7\n",
      "you 7\n",
      "the 7\n",
      "my 6\n",
      "to 6\n",
      "----------------------------\n",
      "Cher's top 5 most common words in song titles:\n",
      "the 29\n",
      "to 28\n",
      "\"the 24\n",
      "of 21\n",
      "\"i 21\n"
     ]
    }
   ],
   "source": [
    "def get_common_words(df, artist_name, title_column):\n",
    "    words_count = Counter()\n",
    "    artist_df = df[df['artist'] == artist_name]\n",
    "    for title in artist_df[title_column]:\n",
    "        words = title.lower().split()\n",
    "        words_count.update(words)\n",
    "\n",
    "    most_common_words = words_count.most_common(5)\n",
    "    return most_common_words\n",
    "\n",
    "#Top 5 most common words in song titles by Robyn\n",
    "most_common_words = get_common_words(lyric_df, 'robyn', 'title')\n",
    "print(\"Robyn's top 5 most common words in song titles:\")\n",
    "for word, count in most_common_words:\n",
    "    print(word, count)\n",
    "\n",
    "print('----------------------------') \n",
    "    \n",
    "#Top 5 most common words in song titles by Cher\n",
    "most_common_words = get_common_words(lyric_df, 'cher', 'title')\n",
    "print(\"Cher's top 5 most common words in song titles:\")\n",
    "for word, count in most_common_words:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Artist 2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD4CAYAAAD7CAEUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZlUlEQVR4nO3dfZBV1Znv8e9PhGAyOMiLkdA4NKk2QvnSIQSxdOJgxhugZiSRygS0hCg1hBGuJvHmpk2mHP3j+jqGjHUJHRKp8SWCxkTTY7jlECNJaQURCSGNBOkwjbQQxZ74wjiKkOf+cXbr4XC6e285u0+//D5Vp3rvtdfavZ5T0E+ttfdeWxGBmZlZWsdVuwNmZta3OHGYmVkmThxmZpaJE4eZmWXixGFmZpkcX+0O9IRRo0bF+PHjq90NM7M+5dlnn30lIkaXlg+IxDF+/Hg2bdpU7W6YmfUpknaXK/dUlZmZZeLEYWZmmThxmJlZJgPiGoeZDWzvvPMObW1tvPXWW9XuSq80dOhQampqGDx4cKr6Thxm1u+1tbUxbNgwxo8fj6Rqd6dXiQja29tpa2ujtrY2VRtPVZlZv/fWW28xcuRIJ40yJDFy5MhMozEnDjMbEJw0Opf1u3HiMDOzTHyNw8wGnGXrnq/o+b5y0Wmp6j388MNccsklbN++ndNPP71snVdffZX777+fq666CoC9e/dy9dVX89BDD6WqX+rKK6/k0Ucf5eSTT6a5uTlVP7vjxGFVU+n/vB3S/ic262mrV6/m/PPPZ82aNdxwww1HHT98+DCvvvoq3/nOd95NBB/5yEc6TRrAUfVLffGLX2Tp0qXMnz+/IjGAp6rMzHrEgQMHeOqpp7jrrrtYs2bNu+Xr169n+vTpXHrppZx55pk0NDTw+9//nvr6er72ta/R2trKGWecAcC2bduYOnUq9fX1nHXWWezcufOo+qU+9alPMWLEiIrG4hGHmVkPeOSRR5gxYwannXYaI0aMYPPmzUyePBmAjRs30tzcTG1tLa2trTQ3N7NlyxYAWltb3z1HY2Mj11xzDZdddhkHDx7k8OHD3HLLLUfU7wkecZiZ9YDVq1czd+5cAObOncvq1avfPTZ16tRUz1Cce+653HTTTdx6663s3r2bE044Ibf+dsUjDjOznLW3t/Pzn/+c5uZmJHH48GEkcdtttwHwoQ99KNV5Lr30Us455xx++tOf8pnPfIbvf//7TJgwIc+ul+URh5lZzh566CHmz5/P7t27aW1tZc+ePdTW1vLkk08eVXfYsGG88cYbZc+za9cuJkyYwNVXX83FF1/M1q1bu6yfF484zGzA6ek771avXk1DQ8MRZXPmzOH+++/nC1/4whHlI0eO5LzzzuOMM85g5syZLFmy5N1jDzzwAPfddx+DBw/mlFNO4frrr2fEiBFH1L/99tuPON+8efNYv349r7zyCjU1Ndx4440sXLjwmOJRRBzTCfqCKVOmhF/k1PvkdTtuXnybb9+1fft2Jk6cWO1u9GrlviNJz0bElNK6nqoyM7NMnDjMzCwTJw4zM8vEicPMzDLJNXFImiFph6QWSQ1ljkvSncnxrZImJ+VDJW2U9BtJ2yTdWNRmhKR1knYmP0/KMwYzMztSbolD0iBgOTATmATMkzSppNpMoC75LAJWJOVvAxdGxNlAPTBD0rTkWAPweETUAY8n+2Zm1kPyfI5jKtASEbsAJK0BZgPPFdWZDdwThXuCN0gaLmlMROwDDiR1BiefKGrzV8n23cB64Os5xmFm/c0TN1f2fNOvS1Wtp5dV37NnD/Pnz+cPf/gDxx13HIsWLeKaa65JGVTn8pyqGgvsKdpvS8pS1ZE0SNIW4GVgXUQ8ndT5cJJYSH6eXPmum5lVXvGy6uUUL6veIe2y6uUcf/zx3HHHHWzfvp0NGzawfPlynnvuubJ1s8gzcZR7F2Hp04ad1omIwxFRD9QAUyWdkemXS4skbZK0af/+/VmamplVXDWWVR8zZsy7K/AOGzaMiRMn8uKLLx5zLHlOVbUB44r2a4C9WetExKuS1gMzgGbgpY7pLEljKIxIjhIRK4GVUHhy/BjiMDM7ZtVeVr21tZVf//rXnHPOOcccS54jjmeAOkm1koYAc4GmkjpNwPzk7qppwGtJQhgtaTiApBOAvwZ+V9RmQbK9APhJjjGYmVVENZdVP3DgAHPmzOHb3/42J5544vsLoEhuI46IOCRpKfAYMAhYFRHbJC1OjjcCa4FZQAvwJnBF0nwMcHdyZ9ZxwIMR8Why7BbgQUkLgReAz+cVg5lZJVRzWfV33nmHOXPmcNlll3HJJZcccyyQ8+q4EbGWQnIoLmss2g5gSZl2W4GPd3LOduDTle2pmVl+OpZV/+53v/tu2QUXXHBMy6rv2rWLrVu3cvbZZ3daPyJYuHAhEydO5Ktf/WplgsHLqpvZQJTy9tlKqday6k899RT33nsvZ555JvX19QDcdNNNzJo165ji8bLqVjVeVt16ipdV756XVTczs9w4cZiZWSZOHGY2IAyEafn3K+t348RhZv3e0KFDaW9vd/IoIyJob29n6NChqdv4rioz6/dqampoa2vDyw+VN3ToUGpqalLXd+Iw648qvfprhx6+jbVSBg8enOrJbEvHU1VmZpaJE4eZmWXiqSqzasprSsksRx5xmJlZJh5xmFl6eYyQ+ugF94HMIw4zM8vEicPMzDJx4jAzs0ycOMzMLBMnDjMzy8SJw8zMMnHiMDOzTJw4zMwsEycOMzPLxE+OWyrL1j1f7S6YWS+R64hD0gxJOyS1SGooc1yS7kyOb5U0OSkfJ+kJSdslbZN0TVGbGyS9KGlL8pmVZwxmZnak3EYckgYBy4GLgDbgGUlNEfFcUbWZQF3yOQdYkfw8BFwbEZslDQOelbSuqO2yiPjnvPpuZmady3PEMRVoiYhdEXEQWAPMLqkzG7gnCjYAwyWNiYh9EbEZICLeALYDY3Psq5mZpZRn4hgL7Cnab+PoP/7d1pE0Hvg48HRR8dJkamuVpJMq1mMzM+tWnolDZcoiSx1Jfwb8CPhyRLyeFK8APgrUA/uAO8r+cmmRpE2SNvkF9WZmlZNn4mgDxhXt1wB709aRNJhC0vhBRPy4o0JEvBQRhyPiT8D3KEyJHSUiVkbElIiYMnr06GMOxszMCvJMHM8AdZJqJQ0B5gJNJXWagPnJ3VXTgNciYp8kAXcB2yPiW8UNJI0p2v0c0JxfCGZmViq3u6oi4pCkpcBjwCBgVURsk7Q4Od4IrAVmAS3Am8AVSfPzgMuB30rakpR9IyLWArdJqqcwpdUKfCmvGMzM7Gi5PgCY/KFfW1LWWLQdwJIy7Z6k/PUPIuLyCnfTzMwy8JIjZmaWiROHmZll4sRhZmaZOHGYmVkmThxmZpaJE4eZmWXixGFmZpn4RU5mKeXxMqtpL7Rz7oSRFT+vWZ484jAzs0ycOMzMLBMnDjMzy8SJw8zMMnHiMDOzTJw4zMwsEycOMzPLxInDzMwyceIwM7NMnDjMzCwTJw4zM8vEicPMzDJx4jAzs0y8Oq6ZVdcTN+dz3unX5XNeSzfikHRG3h0xM7O+Ie1UVaOkjZKukjQ87cklzZC0Q1KLpIYyxyXpzuT4VkmTk/Jxkp6QtF3SNknXFLUZIWmdpJ3Jz5PS9sfMzI5dqsQREecDlwHjgE2S7pd0UVdtJA0ClgMzgUnAPEmTSqrNBOqSzyJgRVJ+CLg2IiYC04AlRW0bgMcjog54PNk3M7MekvrieETsBP4R+DpwAXCnpN9JuqSTJlOBlojYFREHgTXA7JI6s4F7omADMFzSmIjYFxGbk9/7BrAdGFvU5u5k+27gs2ljMDOzY5f2GsdZkpZR+AN+IfC3yWjgQmBZJ83GAnuK9tt4749/6jqSxgMfB55Oij4cEfsAkp8nd9LnRZI2Sdq0f//+rgM0M7PU0o44/i+wGTg7IpYUjQb2UhiFlKMyZZGljqQ/A34EfDkiXk/ZV5K+rYyIKRExZfTo0VmamplZF9LejjsL+O+IOAwg6ThgaES8GRH3dtKmjcI1kQ41wN60dSQNppA0fhARPy6q81LHdJakMcDLKWMwM7MKSDvi+BlwQtH+B5OyrjwD1EmqlTQEmAs0ldRpAuYnd1dNA15LEoKAu4DtEfGtMm0WJNsLgJ+kjMHMzCog7YhjaEQc6NiJiAOSPthVg4g4JGkp8BgwCFgVEdskLU6ONwJrKYxmWoA3gSuS5ucBlwO/lbQlKftGRKwFbgEelLQQeAH4fMoYzMysAtImjv+SNLnj2oakTwD/3V2j5A/92pKyxqLtAJaUafck5a9/EBHtwKdT9tvMzCosbeL4MvBDSR3XKMYAX8ilR2Zm1qulShwR8Yyk04GPURgJ/C4i3sm1Z2Zm1itlWeTwk8D4pM3HJRER9+TSKzMz67VSJQ5J9wIfBbYAh5PiAJw4zMwGmLQjjinApORitpmZDWBpn+NoBk7JsyNmZtY3pB1xjAKek7QReLujMCIuzqVXZmbWa6VNHDfk2QkzM+s70t6O+wtJfwHURcTPkqfGB+XbNTMz643S3lX19xRetDSCwt1VY4FG/AS3DRDTXlhZ7S6Y9RppL44vobB+1Ovw7kudyr4Hw8zM+re0iePt5C1+AEg6nqPfrWFmZgNA2sTxC0nfAE5I3jX+Q+Df8uuWmZn1VmkTRwOwH/gt8CUKK9529uY/MzPrx9LeVfUn4HvJx8zMBrC0d1X9B2WuaUTEhIr3yMzMerUsa1V1GErhrXsjKt8dMzPr7VJd44iI9qLPixHxbeDCfLtmZma9UdqpqslFu8dRGIEMy6VHZmbWq6WdqrqjaPsQ0Ar8XcV7Y2ZmvV7au6qm590RMzPrG9JOVX21q+MR8a3KdMfMzHq7LHdVfRJoSvb/FvglsCePTpmZWe+V9snxUcDkiLg2Iq4FPgHURMSNEXFjZ40kzZC0Q1KLpIYyxyXpzuT41uKL8JJWSXpZUnNJmxskvShpS/KZlTIGMzOrgLSJ41TgYNH+QWB8Vw0kDQKWAzOBScA8SZNKqs0E6pLPImBF0bF/BWZ0cvplEVGffNamjMHMzCog7VTVvcBGSQ9TeIL8c8A93bSZCrRExC4ASWuA2cBzRXVmA/dERAAbJA2XNCYi9kXELyWNzxCLmZn1gLQPAP4f4Argj8CrwBURcVM3zcZy5DWQtqQsa51yliZTW6sknVSugqRFkjZJ2rR///4UpzQzszTSTlUBfBB4PSL+BWiTVNtNfZUpK13vKk2dUisovIWwHtjHkc+YvHeSiJURMSUipowePbqbU5qZWVqpEoekfwK+DlyXFA0G7uumWRswrmi/Btj7PuocISJeiojDRSv2Tu2mH2ZmVkFpRxyfAy4G/gsgIvbS/ZIjzwB1kmolDQHm8t7tvB2agPnJ3VXTgNciYl9XJ5U0pqRfzZ3VNTOzykt7cfxgRISkAJD0oe4aRMQhSUuBx4BBwKqI2CZpcXK8kcILoWYBLcCbFK6jkPyO1cBfAaMktQH/FBF3AbdJqqcwpdVK4cVSZmbWQ9ImjgclfRcYLunvgStJ8VKn5FbZtSVljUXbASzppO28TsovT9lnMzPLQbeJQ5KAB4DTgdeBjwHXR8S6nPtmZma9ULeJI5mieiQiPgE4WZiZDXBpL45vkPTJXHtiZmZ9QtprHNOBxZJaKdxZJQqDkbPy6piZmfVOXSYOSadGxAsU1pQyMzPrdsTxCIVVcXdL+lFEzOmBPpmZWS/W3TWO4iVBJuTZETMz6xu6SxzRybaZmQ1Q3U1VnS3pdQojjxOSbXjv4viJufbOzMx6nS4TR0QM6qmOmJlZ35BlWXUzM7PUz3FYH7Fs3fPV7oKZ9XMecZiZWSZOHGZmlomnqsyq7Fe72it+znMnjKz4Oc06eMRhZmaZOHGYmVkmThxmZpaJE4eZmWXii+Nm1j89cXM+551+XT7n7UM84jAzs0ycOMzMLBMnDjMzyyTXxCFphqQdklokNZQ5Lkl3Jse3SppcdGyVpJclNZe0GSFpnaSdyc+T8ozBzMyOlFvikDQIWE7hfeWTgHmSJpVUmwnUJZ9FwIqiY/8KzChz6gbg8YioAx5P9s3MrIfkOeKYCrRExK6IOAisAWaX1JkN3BMFG4DhksYARMQvgf8sc97ZwN3J9t3AZ/PovJmZlZdn4hgL7Cnab0vKstYp9eGI2AeQ/Dy5XCVJiyRtkrRp//79mTpuZmadyzNxqExZ6XvL09R5XyJiZURMiYgpo0ePrsQpzcyMfBNHGzCuaL8G2Ps+6pR6qWM6K/n58jH208zMMsgzcTwD1EmqlTQEmAs0ldRpAuYnd1dNA17rmIbqQhOwINleAPykkp02M7Ou5ZY4IuIQsBR4DNgOPBgR2yQtlrQ4qbYW2AW0AN8DrupoL2k18CvgY5LaJC1MDt0CXCRpJ3BRsm9mZj0k17WqImItheRQXNZYtB3Akk7azuukvB34dAW7aWZmGfjJcTMzy8SJw8zMMnHiMDOzTJw4zMwsEycOMzPLxInDzMwyceIwM7NMnDjMzCwTJw4zM8vEicPMzDJx4jAzs0ycOMzMLBMnDjMzy8SJw8zMMnHiMDOzTJw4zMwsEycOMzPLxInDzMwyceIwM7NMnDjMzCwTJw4zM8vk+Gp3wKySpr2wstpdMOv3POIwM7NMck0ckmZI2iGpRVJDmeOSdGdyfKukyd21lXSDpBclbUk+s/KMwczMjpRb4pA0CFgOzAQmAfMkTSqpNhOoSz6LgBUp2y6LiPrkszavGMzM7Gh5XuOYCrRExC4ASWuA2cBzRXVmA/dERAAbJA2XNAYYn6Jtn7Zs3fPV7oKZ2fuSZ+IYC+wp2m8DzklRZ2yKtkslzQc2AddGxB9Lf7mkRRRGMZx66qnvMwQzsxJP3JzPeadfl895c5DnNQ6VKYuUdbpquwL4KFAP7APuKPfLI2JlREyJiCmjR49O1WEzM+teniOONmBc0X4NsDdlnSGdtY2IlzoKJX0PeLRyXTYzs+7kOeJ4BqiTVCtpCDAXaCqp0wTMT+6umga8FhH7umqbXAPp8DmgOccYzMysRG4jjog4JGkp8BgwCFgVEdskLU6ONwJrgVlAC/AmcEVXbZNT3yapnsLUVSvwpbxiMDOzo+X65Hhyq+zakrLGou0AlqRtm5RfXuFumplZBn5y3MzMMvFaVWb90K92tedy3nMnjMzlvNa3eMRhZmaZOHGYmVkmThxmZpaJE4eZmWXixGFmZpk4cZiZWSZOHGZmlokTh5mZZeLEYWZmmfjJcauaaS+srHYXzOx98IjDzMwyceIwM7NMnDjMzCwTJw4zM8vEicPMzDJx4jAzs0x8O66ZWW/wxM35nHf6dRU/pUccZmaWiROHmZll4qmqbixb93y1u2Bm1qt4xGFmZpnkOuKQNAP4F2AQ8P2IuKXkuJLjs4A3gS9GxOau2koaATwAjAdagb+LiD/mGYeZFfxqV3su5z13wshczmv5yG3EIWkQsByYCUwC5kmaVFJtJlCXfBYBK1K0bQAej4g64PFk38zMekieU1VTgZaI2BURB4E1wOySOrOBe6JgAzBc0phu2s4G7k627wY+m2MMZmZWIs+pqrHAnqL9NuCcFHXGdtP2wxGxDyAi9kk6udwvl7SIwigG4ICkHe8niB40Cnil2p2oIsfv+B1/Lr5xLI3/olxhnolDZcoiZZ00bbsUESuBPvPCB0mbImJKtftRLY7f8Tv+vhN/nlNVbcC4ov0aYG/KOl21fSmZziL5+XIF+2xmZt3IM3E8A9RJqpU0BJgLNJXUaQLmq2Aa8FoyDdVV2yZgQbK9APhJjjGYmVmJ3KaqIuKQpKXAYxRuqV0VEdskLU6ONwJrKdyK20LhdtwrumqbnPoW4EFJC4EXgM/nFUMP6zPTajlx/AOb4+9DFJHp0oGZmQ1wfnLczMwyceIwM7NMnDh6gKRVkl6W1FxUNkLSOkk7k58nFR27TlKLpB2SPlOdXldOJ/HfLul3krZKeljS8KJj/Sp+KP8dFB37X5JC0qiisn71HXQWv6T/mcS4TdJtReX9Pn5J9ZI2SNoiaZOkqUXHenf8EeFPzh/gU8BkoLmo7DagIdluAG5NticBvwE+ANQCvwcGVTuGHOL/H8Dxyfat/Tn+zr6DpHwchZtAdgOj+ut30Mm/genAz4APJPsnD7D4/x2YmWzPAtb3lfg94ugBEfFL4D9LijtbOmU2sCYi3o6I/6Bwx9lU+rBy8UfEv0fEoWR3A4VndaAfxg+d/hsAWAb8b458wLXffQedxP8PwC0R8XZSp+OZrIESfwAnJtt/znvPqvX6+J04queIpVOAjqVTOluGpT+7Evh/yfaAiV/SxcCLEfGbkkMD5Ts4DfhLSU9L+oWkTyblAyX+LwO3S9oD/DPQ8Y7XXh+/E0fvc8zLrfQlkr4JHAJ+0FFUplq/i1/SB4FvAteXO1ymrN99BxSeIzsJmAZ8jcLzWWLgxP8PwFciYhzwFeCupLzXx+/EUT2dLZ2SZqmWfkHSAuBvgMsimdxl4MT/UQrz17+R1Eohzs2STmHgfAdtwI+jYCPwJwqL/Q2U+BcAP062f8h701G9Pn4njurpbOmUJmCupA9IqqXwrpKNVehfrpIXdX0duDgi3iw6NCDij4jfRsTJETE+IsZT+GMxOSL+wAD5DoBHgAsBJJ0GDKGwQuxAiX8vcEGyfSGwM9nu/fFX++r8QPgAq4F9wDsU/kAsBEZSeBHVzuTniKL636RwJ8UOkrsu+vKnk/hbKMzjbkk+jf01/s6+g5LjrSR3VfXH76CTfwNDgPuAZmAzcOEAi/984FkKd1A9DXyir8TvJUfMzCwTT1WZmVkmThxmZpaJE4eZmWXixGFmZpk4cZiZWSZOHGZmlokTh5mZZfL/AcUMw/NTvWf3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: It matches one or more consecutive whitespace characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "39b3bb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "# Create function to tokenize lyrics\n",
    "def tokenize_lyrics(lyric):\n",
    "    \"\"\"Strip and split on whitespace\"\"\"\n",
    "    return [item.lower() for item in collapse_whitespace.split(lyric)]\n",
    "\n",
    "# Tokenize lyrics and count number of tokens\n",
    "lyric_df['tokenized_lyrics'] = lyric_df['lyrics'].apply(tokenize_lyrics)\n",
    "lyric_df['song_length'] = lyric_df['tokenized_lyrics'].apply(len)\n",
    "\n",
    "# Create histogram for each artist\n",
    "fig, ax = plt.subplots()\n",
    "lyric_df.groupby('artist')['song_length'].plot(kind='hist', density=True, alpha=0.5, legend=True, ax=ax)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Song Length')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Song Lengths by Artist')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8c8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
